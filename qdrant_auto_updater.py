"""Qdrant è‡ªåŠ¨ç´¢å¼•æ›´æ–°å™¨ - å®æ—¶æ–‡ä»¶ç›‘æ§ç‰ˆ
åŸºäº watchdog ç›‘æ§ä»£ç åº“æ–‡ä»¶å˜åŒ–ï¼Œå®æ—¶æ›´æ–° Qdrant å‘é‡ç´¢å¼•ã€‚

ä½¿ç”¨åœºæ™¯ï¼š
- å¼€å‘ç¯å¢ƒï¼šä»£ç ä¿å­˜å³æ›´æ–°ç´¢å¼•
- å®æ—¶åä½œï¼šå¤šäººåŒæ—¶ä¿®æ”¹ä»£ç æ—¶ä¿æŒç´¢å¼•åŒæ­¥
- è°ƒè¯•é˜¶æ®µï¼šå¿«é€ŸéªŒè¯ä»£ç æœç´¢æ•ˆæœ

æŠ€æœ¯æ ˆï¼š
- watchdog: è·¨å¹³å°æ–‡ä»¶ç³»ç»Ÿäº‹ä»¶ç›‘æ§
- Qdrant: å‘é‡æ•°æ®åº“ï¼ˆå¢é‡ upsertï¼‰
- SentenceTransformer: ä»£ç åµŒå…¥æ¨¡å‹

ä½¿ç”¨æ–¹å¼ï¼š
    python qdrant_auto_updater.py \
        --repo /path/to/repo \
        --qdrant-path ./qdrant_storage \
        --collection codebase \
        --debounce 3  # 3 ç§’é˜²æŠ–

ç‰¹æ€§ï¼š
- âœ… çœŸæ­£çš„å®æ—¶æ›´æ–°ï¼ˆæ–‡ä»¶ä¿å­˜å³ç´¢å¼•ï¼‰
- âœ… æ™ºèƒ½é˜²æŠ–ï¼ˆé¿å…é¢‘ç¹è§¦å‘ï¼‰
- âœ… å¢é‡æ›´æ–°ï¼ˆåªç´¢å¼•å˜åŒ–çš„æ–‡ä»¶ï¼‰
- âœ… å“ˆå¸Œæ ¡éªŒï¼ˆé¿å…é‡å¤ç´¢å¼•ï¼‰
"""

from __future__ import annotations

import argparse
import asyncio
import hashlib
import json
import logging
import time
from pathlib import Path
from typing import Dict, Set

from qdrant_client import QdrantClient
from qdrant_client.http import models as rest
from sentence_transformers import SentenceTransformer
from watchdog.events import FileSystemEvent, FileSystemEventHandler
from watchdog.observers import Observer

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
SUPPORTED_EXTENSIONS = {
    ".py", ".rs", ".go", ".js", ".jsx", ".ts", ".tsx", ".java", ".kt", ".swift",
    ".c", ".cc", ".cpp", ".h", ".hpp", ".xml", ".json"
}


class QdrantAutoUpdater(FileSystemEventHandler):
    """å®æ—¶æ–‡ä»¶ç›‘æ§ + è‡ªåŠ¨ç´¢å¼•æ›´æ–°"""

    def __init__(
        self,
        repo_path: Path,
        qdrant_path: Path,
        collection: str,
        model_name: str,
        debounce_seconds: float = 2.0,
    ):
        self.repo_path = repo_path.resolve()
        self.qdrant_path = qdrant_path.resolve()
        self.collection = collection
        self.model_name = model_name
        self.debounce_seconds = debounce_seconds

        # Qdrant å®¢æˆ·ç«¯
        self.client = QdrantClient(path=str(qdrant_path))

        # åµŒå…¥æ¨¡å‹
        logger.info(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

        # æ–‡ä»¶å“ˆå¸Œç¼“å­˜
        self.hash_cache_file = qdrant_path / f"{collection}_watcher_hashes.json"
        self.file_hashes: Dict[str, str] = self._load_hash_cache()

        # é˜²æŠ–æœºåˆ¶ï¼šè®°å½•å¾…å¤„ç†çš„æ–‡ä»¶
        self.pending_files: Set[Path] = set()
        self.last_trigger_time: float = 0

        # æ£€æŸ¥é›†åˆ
        self._check_collection()

    def _check_collection(self) -> None:
        """æ£€æŸ¥ Qdrant é›†åˆæ˜¯å¦å­˜åœ¨"""
        collections = {c.name for c in self.client.get_collections().collections}
        if self.collection not in collections:
            raise RuntimeError(
                f"Collection '{self.collection}' not found. Run qdrant_codebase_indexer.py first."
            )
        logger.info(f"âœ“ é›†åˆå·²æ‰¾åˆ°: {self.collection}")

    def _load_hash_cache(self) -> Dict[str, str]:
        """åŠ è½½æ–‡ä»¶å“ˆå¸Œç¼“å­˜"""
        if self.hash_cache_file.exists():
            try:
                with open(self.hash_cache_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½å“ˆå¸Œç¼“å­˜: {e}")
        return {}

    def _save_hash_cache(self) -> None:
        """ä¿å­˜æ–‡ä»¶å“ˆå¸Œç¼“å­˜"""
        try:
            with open(self.hash_cache_file, 'w') as f:
                json.dump(self.file_hashes, f, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}")

    def _compute_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶ MD5 å“ˆå¸Œ"""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                hasher.update(f.read())
            return hasher.hexdigest()
        except Exception:
            return ""

    def _should_process_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«å¤„ç†"""
        if not file_path.exists() or not file_path.is_file():
            return False

        if file_path.suffix not in SUPPORTED_EXTENSIONS:
            return False

        # è·³è¿‡éšè—ç›®å½•
        if any(part.startswith(".") for part in file_path.parts):
            return False

        # æ£€æŸ¥å“ˆå¸Œ
        current_hash = self._compute_file_hash(file_path)
        if not current_hash:
            return False

        file_str = str(file_path)
        if file_str in self.file_hashes and self.file_hashes[file_str] == current_hash:
            return False  # æ–‡ä»¶æœªå˜åŒ–

        return True

    def _extract_fragments(self, file_path: Path, window: int = 80, stride: int = 60):
        """æå–ä»£ç ç‰‡æ®µï¼ˆæ»‘åŠ¨çª—å£ï¼‰"""
        try:
            text = file_path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            return []

        lines = text.splitlines()
        total_lines = len(lines)
        fragments = []
        idx = 0

        while idx < total_lines:
            segment = lines[idx : idx + window]
            if segment:
                snippet = "\n".join(segment)
                start = idx + 1
                end = min(idx + window, total_lines)

                frag_id = hashlib.md5(f"{file_path}:{start}:{end}".encode("utf-8")).hexdigest()

                fragments.append({
                    "id": frag_id,
                    "text": snippet,
                    "abs_path": str(file_path),
                    "rel_path": str(file_path.relative_to(self.repo_path)),
                    "language": file_path.suffix.lstrip("."),
                    "start_line": start,
                    "end_line": end,
                })
            idx += stride

        return fragments

    def _index_file(self, file_path: Path) -> int:
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {file_path.relative_to(self.repo_path)}")

            # æå–ç‰‡æ®µ
            fragments = self._extract_fragments(file_path)
            if not fragments:
                logger.warning(f"   æœªæå–åˆ°ä»£ç ç‰‡æ®µï¼Œè·³è¿‡")
                return 0

            # ç”Ÿæˆå‘é‡
            texts = [frag["text"] for frag in fragments]
            embeddings = self.model.encode(
                texts,
                batch_size=64,
                show_progress_bar=False,
                normalize_embeddings=True
            )

            # å¢é‡ upsert åˆ° Qdrant
            self.client.upsert(
                collection_name=self.collection,
                points=rest.Batch(
                    ids=[frag["id"] for frag in fragments],
                    vectors=embeddings.tolist(),
                    payloads=[{
                        "path": frag["rel_path"],
                        "abs_path": frag["abs_path"],
                        "language": frag["language"],
                        "start_line": frag["start_line"],
                        "end_line": frag["end_line"],
                    } for frag in fragments],
                ),
            )

            # æ›´æ–°å“ˆå¸Œç¼“å­˜
            file_str = str(file_path)
            self.file_hashes[file_str] = self._compute_file_hash(file_path)

            logger.info(f"   âœ… å·²æ›´æ–° {len(fragments)} ä¸ªä»£ç ç‰‡æ®µ")
            return len(fragments)

        except Exception as e:
            logger.error(f"   âŒ ç´¢å¼•å¤±è´¥: {e}")
            return 0

    def process_pending_files(self) -> None:
        """å¤„ç†å¾…å¤„ç†çš„æ–‡ä»¶é˜Ÿåˆ—"""
        if not self.pending_files:
            return

        current_time = time.time()
        if current_time - self.last_trigger_time < self.debounce_seconds:
            # è¿˜åœ¨é˜²æŠ–æœŸé—´ï¼Œç¨åå†å¤„ç†
            return

        logger.info(f"ğŸ”„ å¤„ç† {len(self.pending_files)} ä¸ªå¾…æ›´æ–°æ–‡ä»¶...")

        total_fragments = 0
        files_to_process = list(self.pending_files)
        self.pending_files.clear()

        for file_path in files_to_process:
            if self._should_process_file(file_path):
                fragments_count = self._index_file(file_path)
                total_fragments += fragments_count

        if total_fragments > 0:
            self._save_hash_cache()
            logger.info(f"âœ… ç´¢å¼•æ›´æ–°å®Œæˆ: {total_fragments} ä¸ªç‰‡æ®µ")

        self.last_trigger_time = current_time

    # ========== Watchdog äº‹ä»¶å¤„ç† ==========

    def on_modified(self, event: FileSystemEvent) -> None:
        """æ–‡ä»¶ä¿®æ”¹äº‹ä»¶"""
        if event.is_directory:
            return

        file_path = Path(event.src_path)
        if file_path.suffix in SUPPORTED_EXTENSIONS:
            logger.debug(f"ğŸ”” æ£€æµ‹åˆ°æ–‡ä»¶ä¿®æ”¹: {file_path.name}")
            self.pending_files.add(file_path)

    def on_created(self, event: FileSystemEvent) -> None:
        """æ–‡ä»¶åˆ›å»ºäº‹ä»¶"""
        if event.is_directory:
            return

        file_path = Path(event.src_path)
        if file_path.suffix in SUPPORTED_EXTENSIONS:
            logger.debug(f"ğŸ”” æ£€æµ‹åˆ°æ–°æ–‡ä»¶: {file_path.name}")
            self.pending_files.add(file_path)

    def start_watching(self) -> None:
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Qdrant è‡ªåŠ¨ç´¢å¼•æ›´æ–°å™¨")
        logger.info("=" * 60)
        logger.info(f"ğŸ“‚ ç›‘æ§ç›®å½•: {self.repo_path}")
        logger.info(f"ğŸ—„ï¸  Qdrant é›†åˆ: {self.collection}")
        logger.info(f"ğŸ§  åµŒå…¥æ¨¡å‹: {self.model_name}")
        logger.info(f"â±ï¸  é˜²æŠ–é—´éš”: {self.debounce_seconds} ç§’")
        logger.info("=" * 60)

        observer = Observer()
        observer.schedule(self, str(self.repo_path), recursive=True)
        observer.start()

        logger.info("ğŸ‘€ æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨ï¼Œç­‰å¾…ä»£ç å˜åŒ–...")
        logger.info("æŒ‰ Ctrl+C åœæ­¢")

        try:
            while True:
                time.sleep(1)
                self.process_pending_files()
        except KeyboardInterrupt:
            logger.info("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·...")
            observer.stop()

        observer.join()
        logger.info("âœ“ ç›‘æ§å·²åœæ­¢")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Qdrant è‡ªåŠ¨ç´¢å¼•æ›´æ–°å™¨ - å®æ—¶æ–‡ä»¶ç›‘æ§",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºç¡€ç”¨æ³•
  python qdrant_auto_updater.py --repo /path/to/repo --collection codebase

  # è‡ªå®šä¹‰é˜²æŠ–æ—¶é—´ï¼ˆ5 ç§’ï¼‰
  python qdrant_auto_updater.py --repo /path/to/repo --collection codebase --debounce 5

ä¾èµ–:
  pip install qdrant-client sentence-transformers watchdog
        """
    )
    parser.add_argument("--repo", required=True, help="Path to repository root")
    parser.add_argument("--qdrant-path", default="./qdrant_storage", help="Qdrant storage directory")
    parser.add_argument("--collection", default="codebase", help="Collection name")
    parser.add_argument("--model", default="sentence-transformers/all-MiniLM-L6-v2", help="Embedding model")
    parser.add_argument("--debounce", type=float, default=2.0, help="Debounce interval in seconds")
    return parser


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    repo_path = Path(args.repo)
    if not repo_path.exists():
        raise SystemExit(f"âŒ Repository not found: {repo_path}")

    updater = QdrantAutoUpdater(
        repo_path=repo_path,
        qdrant_path=Path(args.qdrant_path),
        collection=args.collection,
        model_name=args.model,
        debounce_seconds=args.debounce,
    )

    updater.start_watching()


if __name__ == "__main__":
    main()