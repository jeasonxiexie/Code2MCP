"""Felo Search Android ä»£ç åº“æ™ºèƒ½ç´¢å¼• MCP æœåŠ¡
åŸºäº Qdrant å‘é‡æ•°æ®åº“çš„è¯­ä¹‰ä»£ç æœç´¢æœåŠ¡ï¼Œä¸º Claude Code æä¾›ä»£ç ç†è§£èƒ½åŠ›ã€‚

æ ¸å¿ƒèƒ½åŠ›ï¼š
- è¯­ä¹‰æœç´¢ï¼šç”¨è‡ªç„¶è¯­è¨€æè¿°éœ€æ±‚ï¼Œæ™ºèƒ½å®šä½ç›¸å…³ä»£ç 
- ç²¾ç¡®è¯»å–ï¼šè·å–å®Œæ•´æ–‡ä»¶æˆ–æŒ‡å®šè¡ŒèŒƒå›´çš„ä»£ç 
- ç´¢å¼•çŠ¶æ€ï¼šæŸ¥çœ‹ä»£ç åº“è¦†ç›–æƒ…å†µå’Œå¥åº·çŠ¶æ€
- ğŸ†• è‡ªåŠ¨æ›´æ–°ï¼šæ£€æµ‹ä»£ç å˜åŒ–å¹¶è‡ªåŠ¨æ›´æ–°ç´¢å¼•

æŠ€æœ¯æ ˆï¼š
- å‘é‡æ•°æ®åº“ï¼šQdrant (æœ¬åœ°æ¨¡å¼)
- åµŒå…¥æ¨¡å‹ï¼šsentence-transformers/all-MiniLM-L6-v2 (384ç»´)
- ç›¸ä¼¼åº¦è®¡ç®—ï¼šä½™å¼¦ç›¸ä¼¼åº¦
- é€šä¿¡åè®®ï¼šSSE (Server-Sent Events)

ä½¿ç”¨æ–¹å¼ï¼š
1. è¿è¡Œç´¢å¼•å™¨å»ºç«‹ä»£ç åº“ç´¢å¼•
2. å¯åŠ¨ MCP æœåŠ¡ï¼ˆæ”¯æŒè‡ªåŠ¨æ›´æ–°ï¼‰ï¼š

    python qdrant_codebase_mcp.py \
        --repo /Users/you/Documents/workspace/felo-search-android \
        --qdrant-path ./qdrant_storage \
        --collection felo_android \
        --port 8890 \
        --auto-update \
        --update-interval 30  # 30 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

3. åœ¨ ~/.claude/mcp.json ä¸­æ³¨å†ŒæœåŠ¡ç«¯ç‚¹ (http://localhost:8890/mcp)

ç´¢å¼•è¦†ç›–ï¼š
- felo-search-android: 7703+ ä»£ç ç‰‡æ®µ
- æ”¯æŒ Kotlin, Java, XML, JSON ç­‰å¤šè¯­è¨€
"""

from __future__ import annotations

import argparse
import asyncio
import hashlib
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from fastmcp import FastMCP
from qdrant_client import QdrantClient
from qdrant_client.http import models as rest
from qdrant_client.http.models import Distance
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
SUPPORTED_EXTENSIONS = {
    ".py", ".rs", ".go", ".js", ".jsx", ".ts", ".tsx", ".java", ".kt", ".swift",
    ".c", ".cc", ".cpp", ".h", ".hpp", ".xml", ".json"
}


class QdrantCodebaseService:
    def __init__(
        self,
        repo_path: Path,
        qdrant_path: Path,
        collection: str,
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        auto_update: bool = False,
        update_interval_minutes: int = 30,
    ) -> None:
        self.repo_path = repo_path.resolve()
        self.qdrant_path = qdrant_path.resolve()
        self.client = QdrantClient(path=str(self.qdrant_path))
        self.collection = collection
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        self.mcp = FastMCP("qdrant-codebase")

        # è‡ªåŠ¨æ›´æ–°é…ç½®
        self.auto_update = auto_update
        self.update_interval = update_interval_minutes * 60  # è½¬æ¢ä¸ºç§’
        self.last_check_time: datetime | None = None
        self.update_task: asyncio.Task | None = None

        # æ–‡ä»¶å“ˆå¸Œç¼“å­˜
        self.hash_cache_file = self.qdrant_path / f"{collection}_file_hashes.json"
        self.file_hashes: Dict[str, str] = self._load_hash_cache()

        self._check_collection()
        self._register_tools()

        if auto_update:
            logger.info(f"ğŸ”„ è‡ªåŠ¨æ›´æ–°å·²å¯ç”¨ï¼Œé—´éš”: {update_interval_minutes} åˆ†é’Ÿ")

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------
    def _check_collection(self) -> None:
        collections = {c.name for c in self.client.get_collections().collections}
        if self.collection not in collections:
            raise RuntimeError(
                f"Collection '{self.collection}' not found. Was the indexer run?"
            )

        info = self.client.get_collection(self.collection)
        vectors = info.config.params.vectors
        if vectors is None or vectors.distance != Distance.COSINE:
            raise RuntimeError(
                "Collection distance must be COSINE to match query embeddings."
            )

    def _encode(self, text: str) -> List[float]:
        vector = self.model.encode([text], normalize_embeddings=True)[0]
        return vector.tolist()

    def _resolve_path(self, payload_path: str | None, payload_abs: str | None) -> Path:
        if payload_abs:
            abs_path = Path(payload_abs)
            if abs_path.exists():
                return abs_path
        if payload_path:
            candidate = (self.repo_path / payload_path).resolve()
            if candidate.exists():
                return candidate
        return self.repo_path / (payload_path or "")

    def _read_snippet(self, payload: Dict[str, Any]) -> str:
        file_path = self._resolve_path(payload.get("path"), payload.get("abs_path"))
        try:
            lines = file_path.read_text(encoding="utf-8").splitlines()
        except FileNotFoundError:
            return "(file not found)"

        start = max(payload.get("start_line", 1) - 1, 0)
        end = payload.get("end_line") or start + 1
        snippet = "\n".join(lines[start:end])
        return snippet

    # ------------------------------------------------------------------
    # Auto-update helpers
    # ------------------------------------------------------------------
    def _load_hash_cache(self) -> Dict[str, str]:
        """åŠ è½½æ–‡ä»¶å“ˆå¸Œç¼“å­˜"""
        if self.hash_cache_file.exists():
            try:
                with open(self.hash_cache_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½å“ˆå¸Œç¼“å­˜: {e}")
        return {}

    def _save_hash_cache(self) -> None:
        """ä¿å­˜æ–‡ä»¶å“ˆå¸Œç¼“å­˜"""
        try:
            with open(self.hash_cache_file, 'w') as f:
                json.dump(self.file_hashes, f, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}")

    def _compute_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶ MD5 å“ˆå¸Œ"""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                hasher.update(f.read())
            return hasher.hexdigest()
        except Exception:
            return ""

    def _iter_source_files(self) -> List[Path]:
        """éå†æºä»£ç æ–‡ä»¶"""
        files = []
        for path in self.repo_path.rglob("*"):
            if not path.is_file():
                continue
            if path.suffix.lower() not in SUPPORTED_EXTENSIONS:
                continue
            # è·³è¿‡éšè—ç›®å½•
            if any(part.startswith(".") for part in path.parts):
                continue
            files.append(path)
        return files

    def _find_modified_files(self) -> List[Path]:
        """æŸ¥æ‰¾ä¿®æ”¹è¿‡çš„æ–‡ä»¶"""
        modified_files = []
        all_files = self._iter_source_files()

        for file_path in all_files:
            file_str = str(file_path)
            current_hash = self._compute_file_hash(file_path)

            if not current_hash:
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
            if file_str not in self.file_hashes or self.file_hashes[file_str] != current_hash:
                modified_files.append(file_path)
                self.file_hashes[file_str] = current_hash

        return modified_files

    def _extract_fragments(self, file_path: Path, window: int = 80, stride: int = 60) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªæ–‡ä»¶æå–ä»£ç ç‰‡æ®µï¼ˆæ»‘åŠ¨çª—å£æ–¹å¼ï¼‰"""
        try:
            text = file_path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            return []

        lines = text.splitlines()
        total_lines = len(lines)
        fragments = []
        idx = 0

        while idx < total_lines:
            segment = lines[idx : idx + window]
            if segment:
                snippet = "\n".join(segment)
                start = idx + 1
                end = min(idx + window, total_lines)

                # ç”Ÿæˆå”¯ä¸€ ID
                frag_id = hashlib.md5(f"{file_path}:{start}:{end}".encode("utf-8")).hexdigest()

                fragments.append({
                    "id": frag_id,
                    "text": snippet,
                    "abs_path": str(file_path),
                    "rel_path": str(file_path.relative_to(self.repo_path)),
                    "language": file_path.suffix.lstrip("."),
                    "start_line": start,
                    "end_line": end,
                })
            idx += stride

        return fragments

    async def _incremental_update(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¢é‡ç´¢å¼•æ›´æ–°"""
        logger.info("ğŸ” æ£€æŸ¥ä»£ç å˜åŒ–...")
        self.last_check_time = datetime.now()

        modified_files = self._find_modified_files()

        if not modified_files:
            logger.info("âœ“ ä»£ç æ— å˜åŒ–")
            return {"updated": False, "files_count": 0, "fragments_count": 0}

        logger.info(f"ğŸ“ å‘ç° {len(modified_files)} ä¸ªæ–‡ä»¶æœ‰æ›´æ–°")

        # æå–æ‰€æœ‰å˜åŒ–æ–‡ä»¶çš„ç‰‡æ®µ
        all_fragments = []
        for file_path in modified_files:
            fragments = self._extract_fragments(file_path)
            all_fragments.extend(fragments)

        if not all_fragments:
            return {"updated": False, "files_count": len(modified_files), "fragments_count": 0}

        # ç”Ÿæˆå‘é‡
        logger.info(f"ğŸ§  ç”Ÿæˆå‘é‡åµŒå…¥ ({len(all_fragments)} ä¸ªç‰‡æ®µ)...")
        texts = [frag["text"] for frag in all_fragments]
        embeddings = self.model.encode(texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)

        # å¢é‡ upsert åˆ° Qdrant
        logger.info("â˜ï¸  æ›´æ–°å‘é‡æ•°æ®åº“...")
        batch_size = 64
        for i in range(0, len(all_fragments), batch_size):
            batch = all_fragments[i : i + batch_size]
            vecs = embeddings[i : i + batch_size].tolist()

            self.client.upsert(
                collection_name=self.collection,
                points=rest.Batch(
                    ids=[frag["id"] for frag in batch],
                    vectors=vecs,
                    payloads=[{
                        "path": frag["rel_path"],
                        "abs_path": frag["abs_path"],
                        "language": frag["language"],
                        "start_line": frag["start_line"],
                        "end_line": frag["end_line"],
                    } for frag in batch],
                ),
            )

        # ä¿å­˜å“ˆå¸Œç¼“å­˜
        self._save_hash_cache()

        logger.info(f"âœ… ç´¢å¼•æ›´æ–°å®Œæˆ: {len(all_fragments)} ä¸ªç‰‡æ®µ")
        return {
            "updated": True,
            "files_count": len(modified_files),
            "fragments_count": len(all_fragments),
            "updated_files": [str(f.relative_to(self.repo_path)) for f in modified_files]
        }

    async def _auto_update_loop(self) -> None:
        """åå°è‡ªåŠ¨æ›´æ–°å¾ªç¯"""
        logger.info(f"ğŸš€ å¯åŠ¨è‡ªåŠ¨æ›´æ–°å¾ªç¯ (é—´éš”: {self.update_interval / 60:.1f} åˆ†é’Ÿ)")

        while True:
            try:
                await asyncio.sleep(self.update_interval)
                result = await self._incremental_update()

                if result["updated"]:
                    logger.info(f"ğŸ”„ è‡ªåŠ¨æ›´æ–°å®Œæˆ: {result['fragments_count']} ä¸ªç‰‡æ®µ")
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨æ›´æ–°å¤±è´¥: {e}")

    # ------------------------------------------------------------------
    # MCP tool registration
    # ------------------------------------------------------------------
    def _register_tools(self) -> None:
        @self.mcp.tool()
        async def search_code(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
            """è¯­ä¹‰æœç´¢ä»£ç åº“ - ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°åŠŸèƒ½éœ€æ±‚ï¼Œæ™ºèƒ½å®šä½ç›¸å…³ä»£ç ç‰‡æ®µã€‚

            é€‚ç”¨åœºæ™¯ï¼š
            - "MQTT æ¶ˆæ¯å‘é€é€»è¾‘" â†’ æ‰¾åˆ°æ¶ˆæ¯å‘å¸ƒã€è®¢é˜…ã€è¿æ¥ç›¸å…³ä»£ç 
            - "ç”¨æˆ·ç™»å½•æµç¨‹" â†’ å®šä½ LoginViewModel, AuthRepository ç­‰
            - "ä¸»é¢˜é¢œè‰²ç®¡ç†" â†’ æ‰¾åˆ° Theme.kt, CustomColor.kt
            - "å¤šæ­¥éª¤ Agent æ‰§è¡Œæµç¨‹" â†’ å®šä½å®Œæ•´çš„æ‰§è¡Œé“¾è·¯

            ä¸ä¼ ç»Ÿå·¥å…·çš„åŒºåˆ«ï¼š
            - Grep: ç²¾ç¡®æ–‡æœ¬åŒ¹é… (å·²çŸ¥å…³é”®å­—)
            - Glob: æ–‡ä»¶åæ¨¡å¼åŒ¹é… (å·²çŸ¥è·¯å¾„)
            - search_code: è¯­ä¹‰ç†è§£æœç´¢ (ç”¨éœ€æ±‚æè¿°æ‰¾å®ç°)
            """

            vector = self._encode(query)
            hits = self.client.search(
                collection_name=self.collection,
                query_vector=vector,
                limit=top_k,
                with_payload=True,
            )

            results: List[Dict[str, Any]] = []
            for hit in hits:
                payload = hit.payload or {}
                snippet = self._read_snippet(payload)
                results.append(
                    {
                        "score": float(hit.score),
                        "path": payload.get("path"),
                        "abs_path": payload.get("abs_path"),
                        "language": payload.get("language"),
                        "start_line": payload.get("start_line"),
                        "end_line": payload.get("end_line"),
                        "snippet": snippet,
                    }
                )
            return results

        @self.mcp.tool()
        async def read_resource(path: str, start_line: int = 1, end_line: int | None = None) -> Dict[str, Any]:
            """ç²¾ç¡®è¯»å–ä»£ç æ–‡ä»¶ - è·å–æŒ‡å®šæ–‡ä»¶çš„å®Œæ•´ä»£ç æˆ–ç‰¹å®šè¡ŒèŒƒå›´ã€‚

            ç”¨é€”ï¼š
            - é…åˆ search_code ä½¿ç”¨ï¼šå…ˆè¯­ä¹‰æœç´¢å®šä½ï¼Œå†ç²¾ç¡®è¯»å–å®Œæ•´ä¸Šä¸‹æ–‡
            - æŸ¥çœ‹å®Œæ•´æ–‡ä»¶å†…å®¹ï¼šä¸æŒ‡å®šè¡Œå·èŒƒå›´
            - è¯»å–ç‰¹å®šä»£ç æ®µï¼šæŒ‡å®š start_line å’Œ end_line

            ç¤ºä¾‹ï¼š
            - read_resource("app/src/main/java/...SearchViewModel.kt") â†’ å®Œæ•´æ–‡ä»¶
            - read_resource("...Theme.kt", start_line=30, end_line=50) â†’ 30-50è¡Œ
            """

            file_path = self._resolve_path(path, None).resolve()
            if not file_path.exists():
                return {"error": f"File not found: {file_path}"}

            text = file_path.read_text(encoding="utf-8")
            if end_line is None and start_line <= 1:
                return {"path": str(file_path), "content": text}

            lines = text.splitlines()
            start = max(start_line - 1, 0)
            end = end_line if end_line is not None else len(lines)
            snippet = "\n".join(lines[start:end])
            return {
                "path": str(file_path),
                "start_line": start_line,
                "end_line": end_line or len(lines),
                "content": snippet,
            }

        @self.mcp.tool()
        async def collection_info() -> Dict[str, Any]:
            """æŸ¥çœ‹ä»£ç ç´¢å¼•çŠ¶æ€ - è¿”å› Qdrant å‘é‡æ•°æ®åº“çš„å¥åº·çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯ã€‚

            åŒ…å«ä¿¡æ¯ï¼š
            - points_count: å·²ç´¢å¼•çš„ä»£ç ç‰‡æ®µæ•°é‡
            - vectors_count: å‘é‡æ•°é‡
            - vector_size: å‘é‡ç»´åº¦ (384ç»´è¯­ä¹‰å‘é‡)
            - distance_metric: è·ç¦»è®¡ç®—æ–¹å¼ (ä½™å¼¦ç›¸ä¼¼åº¦)
            - collection_status: é›†åˆå¥åº·çŠ¶æ€

            ç”¨é€”ï¼š
            - æ£€æŸ¥ç´¢å¼•æ˜¯å¦å®Œæ•´
            - äº†è§£ä»£ç åº“è¦†ç›–èŒƒå›´
            - è¯Šæ–­æœç´¢é—®é¢˜
            """

            info = self.client.get_collection(self.collection)
            description = info.dict()
            return json.loads(json.dumps(description, default=str))

        @self.mcp.tool()
        async def trigger_index_update() -> Dict[str, Any]:
            """ğŸ”„ æ‰‹åŠ¨è§¦å‘ç´¢å¼•æ›´æ–° - ç«‹å³æ£€æŸ¥ä»£ç å˜åŒ–å¹¶æ›´æ–°å‘é‡åº“ã€‚

            ç”¨é€”ï¼š
            - åœ¨å¤§é‡ä¿®æ”¹ä»£ç åç«‹å³åŒæ­¥ç´¢å¼•
            - ä¸æƒ³ç­‰å¾…è‡ªåŠ¨æ›´æ–°é—´éš”æ—¶æ‰‹åŠ¨è§¦å‘
            - éªŒè¯ç´¢å¼•æ›´æ–°åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ

            è¿”å›ï¼š
            - updated: æ˜¯å¦æœ‰æ›´æ–°
            - files_count: æ›´æ–°çš„æ–‡ä»¶æ•°é‡
            - fragments_count: æ›´æ–°çš„ä»£ç ç‰‡æ®µæ•°é‡
            - updated_files: æ›´æ–°çš„æ–‡ä»¶åˆ—è¡¨

            ç¤ºä¾‹ä½¿ç”¨åœºæ™¯ï¼š
            - ç”¨æˆ·: "æˆ‘åˆšä¿®æ”¹äº† LoginViewModelï¼Œå¸®æˆ‘æ›´æ–°ä¸€ä¸‹ç´¢å¼•"
            - Claude: [è°ƒç”¨ trigger_index_update] ç´¢å¼•å·²æ›´æ–°ï¼ŒåŒ…å« 3 ä¸ªæ–‡ä»¶çš„ 42 ä¸ªä»£ç ç‰‡æ®µ
            """

            logger.info("ğŸ“¢ æ”¶åˆ°æ‰‹åŠ¨æ›´æ–°è¯·æ±‚")
            result = await self._incremental_update()
            return result

        @self.mcp.tool()
        async def index_status() -> Dict[str, Any]:
            """ğŸ“Š æŸ¥çœ‹ç´¢å¼•æ›´æ–°çŠ¶æ€ - è¿”å›è‡ªåŠ¨æ›´æ–°é…ç½®å’Œè¿è¡ŒçŠ¶æ€ã€‚

            åŒ…å«ä¿¡æ¯ï¼š
            - auto_update_enabled: è‡ªåŠ¨æ›´æ–°æ˜¯å¦å¯ç”¨
            - update_interval_minutes: æ›´æ–°æ£€æŸ¥é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
            - last_check_time: æœ€åæ£€æŸ¥æ—¶é—´
            - next_check_in_seconds: è·ç¦»ä¸‹æ¬¡æ£€æŸ¥çš„ç§’æ•°
            - total_indexed_files: å·²ç´¢å¼•çš„æ–‡ä»¶æ€»æ•°
            - total_vectors: å‘é‡æ€»æ•°

            ç”¨é€”ï¼š
            - äº†è§£è‡ªåŠ¨æ›´æ–°é…ç½®
            - æŸ¥çœ‹æœ€åæ›´æ–°æ—¶é—´
            - é¢„ä¼°ä¸‹æ¬¡æ›´æ–°æ—¶é—´
            - è°ƒè¯•ç´¢å¼•é—®é¢˜

            ç¤ºä¾‹ä½¿ç”¨åœºæ™¯ï¼š
            - ç”¨æˆ·: "ç´¢å¼•å¤šä¹…æ›´æ–°ä¸€æ¬¡ï¼Ÿ"
            - Claude: [è°ƒç”¨ index_status] è‡ªåŠ¨æ›´æ–°å·²å¯ç”¨ï¼Œæ¯ 30 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼Œä¸Šæ¬¡æ£€æŸ¥åœ¨ 15 åˆ†é’Ÿå‰
            """

            info = self.client.get_collection(self.collection)

            next_check_seconds = None
            if self.auto_update and self.last_check_time:
                elapsed = (datetime.now() - self.last_check_time).total_seconds()
                next_check_seconds = max(0, self.update_interval - elapsed)

            return {
                "auto_update_enabled": self.auto_update,
                "update_interval_minutes": self.update_interval / 60,
                "last_check_time": self.last_check_time.isoformat() if self.last_check_time else None,
                "next_check_in_seconds": next_check_seconds,
                "total_indexed_files": len(self.file_hashes),
                "total_vectors": info.points_count,
                "model_name": self.model_name,
                "collection_name": self.collection,
            }

    # ------------------------------------------------------------------
    # Runner
    # ------------------------------------------------------------------
    def serve(self, port: int) -> None:
        async def run_service():
            host = "0.0.0.0"
            logger.info(f"ğŸš€ å¯åŠ¨ Qdrant Codebase MCP æœåŠ¡")
            logger.info(f"   æœåŠ¡åœ°å€: http://{host}:{port}/mcp")
            logger.info(f"   ä»£ç åº“: {self.repo_path}")
            logger.info(f"   é›†åˆ: {self.collection}")
            logger.info(f"   æ¨¡å‹: {self.model_name}")

            if self.auto_update:
                logger.info(f"   è‡ªåŠ¨æ›´æ–°: å·²å¯ç”¨ (é—´éš”: {self.update_interval / 60:.0f} åˆ†é’Ÿ)")
                # å¯åŠ¨åå°æ›´æ–°ä»»åŠ¡
                self.update_task = asyncio.create_task(self._auto_update_loop())
            else:
                logger.info("   è‡ªåŠ¨æ›´æ–°: æœªå¯ç”¨")

            logger.info("=" * 60)
            logger.info("å¯ç”¨å·¥å…·:")
            logger.info("  - search_code: è¯­ä¹‰æœç´¢ä»£ç ")
            logger.info("  - read_resource: è¯»å–æ–‡ä»¶å†…å®¹")
            logger.info("  - collection_info: æŸ¥çœ‹ç´¢å¼•çŠ¶æ€")
            logger.info("  - trigger_index_update: æ‰‹åŠ¨è§¦å‘æ›´æ–°")
            logger.info("  - index_status: æŸ¥çœ‹æ›´æ–°çŠ¶æ€")
            logger.info("=" * 60)

            # ä½¿ç”¨ SSE ä¼ è¾“æ¨¡å¼
            await self.mcp.run(transport="sse", host=host, port=port)

        asyncio.run(run_service())


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Run MCP server backed by Qdrant with auto-update support",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯åŠ¨æœåŠ¡ï¼ˆæ— è‡ªåŠ¨æ›´æ–°ï¼‰
  python qdrant_codebase_mcp.py --repo /path/to/repo --collection codebase

  # å¯åŠ¨æœåŠ¡å¹¶å¯ç”¨è‡ªåŠ¨æ›´æ–°ï¼ˆæ¯ 30 åˆ†é’Ÿï¼‰
  python qdrant_codebase_mcp.py --repo /path/to/repo --collection codebase --auto-update

  # è‡ªå®šä¹‰æ›´æ–°é—´éš”ï¼ˆæ¯ 10 åˆ†é’Ÿï¼‰
  python qdrant_codebase_mcp.py --repo /path/to/repo --collection codebase --auto-update --update-interval 10
        """
    )
    parser.add_argument("--repo", required=True, help="Path to repository root used during indexing")
    parser.add_argument("--qdrant-path", default="./qdrant_storage", help="Directory where Qdrant local data lives")
    parser.add_argument("--collection", default="codebase", help="Collection name")
    parser.add_argument("--model", default="sentence-transformers/all-MiniLM-L6-v2", help="Embedding model (must match indexer)")
    parser.add_argument("--port", type=int, default=8890, help="HTTP port for MCP server")
    parser.add_argument("--auto-update", action="store_true", help="Enable automatic index updates")
    parser.add_argument("--update-interval", type=int, default=30, help="Auto-update interval in minutes (default: 30)")
    return parser


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    repo_path = Path(args.repo)
    if not repo_path.exists():
        raise SystemExit(f"Repository not found: {repo_path}")

    service = QdrantCodebaseService(
        repo_path=repo_path,
        qdrant_path=Path(args.qdrant_path),
        collection=args.collection,
        model_name=args.model,
        auto_update=args.auto_update,
        update_interval_minutes=args.update_interval,
    )
    service.serve(args.port)


if __name__ == "__main__":
    main()
